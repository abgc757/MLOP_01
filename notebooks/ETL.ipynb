{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import ast\n",
    "import re\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga `australian_user_reviewes.json` en el DataFrame `reviews`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta funcion reemplaza \"NaN\" por \"\\\"NaN\\\"\"\n",
    "def reemplazar_nan_con_none(cadena):\n",
    "    resultado = \"\"\n",
    "    indice = 0\n",
    "    longitud = len(cadena)\n",
    "\n",
    "    while indice < longitud:\n",
    "        if cadena[indice:indice+3] == \"NaN\":\n",
    "            resultado += \"\\\"NaN\\\"\"\n",
    "            indice += 3\n",
    "        else:\n",
    "            resultado += cadena[indice]\n",
    "            indice += 1\n",
    "\n",
    "    return resultado\n",
    "\n",
    "# Abre `australian_user_reviewes.json`, y crea el DataFrame `reviews`\n",
    "\n",
    "with open('../dataset/australian_user_reviews.json', 'r', encoding='utf-8') as file:\n",
    "    data_list = []\n",
    "    for linea in file:\n",
    "        linea = reemplazar_nan_con_none(linea)\n",
    "        data = ast.literal_eval(linea.strip())\n",
    "        if isinstance(data, dict):\n",
    "            data_list.append(data)\n",
    "    reviews = pd.DataFrame(data_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función analisis de sentimiento\n",
    "Creación de columna `sentiment_analysis` aplicando análisis de sentimiento con NLP con la escala:\n",
    "\n",
    "- `0` si es malo o reseña ausente\n",
    "- `1` si es neutral\n",
    "- `2` si es positivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "def analyze_sentiment(lista):\n",
    "    nlist =[]\n",
    "    for i in lista:\n",
    "        sentiment = sia.polarity_scores(i['review'])    \n",
    "        try:               \n",
    "            if sentiment['compound'] >= 0.05:\n",
    "                i['sentiment_analysis'] = 2  # Positivo\n",
    "                #del i['review']\n",
    "            elif sentiment['compound'] <= -0.05:\n",
    "                i['sentiment_analysis'] = 0  # Malo\n",
    "                #del i['review']\n",
    "            else:\n",
    "                i['sentiment_analysis'] = 1  # Neutral\n",
    "                #del i['review']\n",
    "        except:\n",
    "            i['sentiment_analysis'] = 1\n",
    "            #del i['review']\n",
    "        nlist.append(i)\n",
    "    return nlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['reviews'] = reviews['reviews'].apply(lambda x: analyze_sentiment(x))   # Aplicando NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desanidar_reviews(df):\n",
    "    i = 0\n",
    "    data_list = []\n",
    "    while i <= len(df['user_id']) -1:\n",
    "        user_id = df['user_id'].iloc[i]\n",
    "        user_url = df['user_url'].iloc[i]\n",
    "        lista = df['reviews'].iloc[i]\n",
    "        for j in lista:\n",
    "            j['user_id'] = user_id\n",
    "            j['user_url'] = user_url\n",
    "            data_list.append(j)\n",
    "        i = i + 1\n",
    "    return data_list\n",
    "\n",
    "n_reviews = pd.DataFrame(desanidar_reviews(reviews))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función para convertir fechas al formato \"YYYY-MM-DD\" para columna `posted`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertir_fecha(fecha):\n",
    "    # Utilizar expresión regular para extraer componentes de la fecha\n",
    "    match = re.match(r\"Posted (\\w+) (\\d+), (\\d+)\", fecha)\n",
    "    if match:\n",
    "        mes_str, dia_str, anio_str = match.groups()\n",
    "        # Mapear nombres de meses a números\n",
    "        meses = {\n",
    "            'January': '01', 'February': '02', 'March': '03', 'April': '04',\n",
    "            'May': '05', 'June': '06', 'July': '07', 'August': '08',\n",
    "            'September': '09', 'October': '10', 'November': '11', 'December': '12'\n",
    "        }\n",
    "        # Crear la fecha en el nuevo formato\n",
    "        nueva_fecha = f\"{anio_str}-{meses[mes_str]}-{dia_str.zfill(2)}\"\n",
    "        return nueva_fecha\n",
    "    else:\n",
    "        return None\n",
    "n_reviews['posted_date'] = n_reviews['posted'].apply(convertir_fecha)\n",
    "n_reviews['posted_date'] = pd.to_datetime(n_reviews['posted_date'])\n",
    "n_reviews = n_reviews.drop(['posted'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función que permite normalizar la fecha para la columna `last_edited`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertir_fecha(fecha):\n",
    "    # Utilizar expresión regular para extraer componentes de la fecha\n",
    "    match = re.match(r\"Last edited (\\w+) (\\d+), (\\d+)\", fecha)\n",
    "    if match:\n",
    "        mes_str, dia_str, anio_str = match.groups()\n",
    "        # Mapear nombres de meses a números\n",
    "        meses = {\n",
    "            'January': '01', 'February': '02', 'March': '03', 'April': '04',\n",
    "            'May': '05', 'June': '06', 'July': '07', 'August': '08',\n",
    "            'September': '09', 'October': '10', 'November': '11', 'December': '12'\n",
    "        }\n",
    "        # Crear la fecha en el nuevo formato\n",
    "        nueva_fecha = f\"{anio_str}-{meses[mes_str]}-{dia_str.zfill(2)}\"\n",
    "        return nueva_fecha\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "n_reviews['last_edited'] = n_reviews['last_edited'].apply(convertir_fecha)\n",
    "n_reviews['last_edited'] = pd.to_datetime(n_reviews['last_edited'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función que permite extraer a la cantidad de personas que cosidenraron graciosa las review de la columna `funny`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_numero(cadena):\n",
    "    match = re.match(r\"([\\d,]+)\", cadena)\n",
    "    if match:\n",
    "        numero_str = match.group(1).replace(',', '')\n",
    "        return int(numero_str)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "n_reviews['funny'] = n_reviews['funny'].apply(extraer_numero)\n",
    "n_reviews['funny'] = n_reviews['funny'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminar columna `user_url`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_reviews = n_reviews.drop(['user_url'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraer votos realizados a las reviews realizadas de la columna `helpful`, se extraen  de la siguiente manera:\n",
    "- `Useful_recommend` Cantidad de votos positivos\n",
    "- `#_recommend` Total de votos\n",
    "- `%_recommend` Porcentaje de votos positivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_numeros(cadena):\n",
    "    match = re.match(r\"(\\d+) of (\\d+) people \\((\\d+)%\\) found this review helpful\", cadena)\n",
    "    if match:\n",
    "        serie = pd.Series({\n",
    "            'Useful_recommend': int(match.group(1)),  # 15\n",
    "            '#_recommend': int(match.group(2)),  # 20\n",
    "            '%_recommend': int(match.group(3))   # 75\n",
    "        })\n",
    "        return serie\n",
    "    else:\n",
    "        return pd.Series({'Useful_recommend': 0, '#_recommend': 0, '%_recommend': 0})\n",
    "\n",
    "\n",
    "n_reviews[['Useful_recommend','#_recommend','%_recommend']] = n_reviews['helpful'].apply(extraer_numeros)\n",
    "n_reviews['%_recommend'] = n_reviews['%_recommend'].astype(float) / 100\n",
    "n_reviews[['Useful_recommend','#_recommend']] = n_reviews[['Useful_recommend','#_recommend']].astype(int)\n",
    "n_reviews = n_reviews.drop(['helpful'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las reviews son almacenadas en el archivo `reviews.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_reviews['item_id'] = n_reviews['item_id'].astype(int)\n",
    "n_reviews.to_parquet('../dataset/reviews.parquet', engine='pyarrow', compression='snappy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creación de DataFrame `df_vectores_reviews` para la función def `recomendacion_juego`. Este DataFrame contiene las recomendaciónes por `item_id` de cada videojuego y se vectoriza por feature words, el resultado se almacena en el archivo `reviews_per_item.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for i in n_reviews.iterrows():\n",
    "    if n_reviews['item_id'].iloc[i[0]] in data:\n",
    "        lista = data[n_reviews['item_id'].iloc[i[0]]]\n",
    "        lista.append(n_reviews['review'].iloc[i[0]])\n",
    "    else:\n",
    "        lista = []\n",
    "        lista.append(n_reviews['review'].iloc[i[0]])\n",
    "        data[n_reviews['item_id'].iloc[i[0]]]=lista\n",
    "reviews = pd.DataFrame(list(data.items()), columns=['id', 'review'])\n",
    "\n",
    "reviews['reviews_concatenadas'] = reviews['review'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)  # Se eligen 1000 palabras claves para vectorizar\n",
    "# Aplicar el vectorizador a los textos de las reviews\n",
    "vectores_reviews = vectorizer.fit_transform(reviews['reviews_concatenadas'])\n",
    "\n",
    "df_vectores_reviews = pd.concat([reviews[['id', 'reviews_concatenadas']], pd.DataFrame(vectores_reviews.toarray(), columns=[f\"feature_{i}\" for i in range(vectores_reviews.shape[1])])], axis=1)\n",
    "df_vectores_reviews = df_vectores_reviews.drop(['reviews_concatenadas'], axis=1)\n",
    "df_vectores_reviews['id'] = df_vectores_reviews['id'].astype(int)\n",
    "# Guardar el DataFrame en formato Parquet\n",
    "df_vectores_reviews.to_parquet('../dataset/reviews_per_item.parquet', engine='pyarrow', compression='snappy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creación de DataFrame que alimentará la función `recomendacion_juego` almacenada en `game_recommend.parquet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_recommend = n_reviews[['item_id','sentiment_analysis']]\n",
    "games_recommend = games_recommend.dropna()\n",
    "games_recommend = games_recommend.fillna(0)\n",
    "games_recommend.to_parquet('../dataset/games_recommend.parquet', engine='pyarrow', compression='snappy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga `output_steam_games.json` en el DataFrame `games`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../dataset/output_steam_games.json', 'r', encoding='utf-8') as file:\n",
    "    data_list = []\n",
    "    for linea in file:\n",
    "        linea = reemplazar_nan_con_none(linea)\n",
    "        data = json.loads(linea.strip())\n",
    "        if isinstance(data, dict):\n",
    "            data_list.append(data)\n",
    "    games = pd.DataFrame(data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminando registros nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = games[games['id'] != 'NaN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normlizando fecha para la columna `release_date`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "games.loc[games['release_date'] == 'Soon..', 'release_date'] = pd.NaT \n",
    "games['release_date'] = pd.to_datetime(games['release_date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputando valor de `0` para cada registro que contenga `str` de la columna `price`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "while i<len(games['price']):\n",
    "    if type(games['price'].iloc[i]) == str:\n",
    "        games['price'] = games['price'].replace(games['price'].iloc[i],0)\n",
    "    i += 1\n",
    "games['price'] = games['price'].replace('NaN', np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La columna `early_access` se formatea como tipo booleano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "games['early_access'] = games['early_access'].astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La columna `id` es formateada como dato tipo entero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "games['id'] = games['id'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El DataFrame es almacenado en el archivo `games.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = games.replace('NaN', np.nan)\n",
    "games.to_parquet('../dataset/games.parquet', engine='pyarrow', compression='snappy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creacion de DataFrame para alimentar la función `UsersRecommend`, el cual es almacenado en `recommend.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_reviews['year'] = n_reviews['posted_date'].dt.year\n",
    "recommend = n_reviews.merge(games, left_on='item_id', right_on='id', how='inner')\n",
    "filtro = recommend[recommend['recommend'] == True]\n",
    "recommend = filtro.groupby(['title','year'])['recommend'].value_counts().reset_index()\n",
    "recommend.to_parquet('../dataset/recommend.parquet', engine='pyarrow', compression='snappy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creacion de DataFrame para la función `UsersNotRecommend`, que es almacenado en `notrecommend.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "notrecommend = n_reviews.merge(games, left_on='item_id', right_on='id', how='inner')\n",
    "notrecommend['not_recommend'] = (notrecommend['recommend'] == False) | (notrecommend['sentiment_analysis'] == 0)\n",
    "filtro1 = notrecommend[notrecommend['not_recommend'] == True]\n",
    "notrecommend = filtro1.groupby(['title','year'])['not_recommend'].value_counts().reset_index()\n",
    "notrecommend.to_parquet('../dataset/notrecommend.parquet', engine='pyarrow', compression='snappy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creación de DataFrame para la función `sentiment_analysis` que es almacenado en el archivo `sentiments.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stdio\\AppData\\Local\\Temp\\ipykernel_2484\\505952917.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sentiments['release_date'] = pd.to_datetime(sentiments['release_date'], format=\"%Y-%m-%d\")\n"
     ]
    }
   ],
   "source": [
    "merged_sentiments = games.merge(n_reviews, left_on='id', right_on='item_id', how='inner')\n",
    "sentiments = merged_sentiments[['release_date','sentiment_analysis']]\n",
    "sentiments['release_date'] = pd.to_datetime(sentiments['release_date'], format=\"%Y-%m-%d\")\n",
    "sentiments.to_parquet('../dataset/sentiments.parquet', engine='pyarrow', compression='snappy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga `australian_users_items.json` en el DataFrame `items`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_valores(cadena):\n",
    "    valores = {}\n",
    "    \n",
    "    # Expresiones regulares para cada clave\n",
    "    patrones = {\n",
    "        'user_id': r'\"user_id\"\\s*:\\s*\"([^\"]+)\"',\n",
    "        'items_count': r'\"items_count\"\\s*:\\s*([^,]+)',\n",
    "        'steam_id': r'\"steam_id\"\\s*:\\s*\"([^\"]+)\"',\n",
    "        'user_url': r'\"user_url\"\\s*:\\s*\"([^\"]+)\"',\n",
    "        'items': r'\"items\"\\s*:\\s*\\{([^}]+)\\}'\n",
    "    }\n",
    "    \n",
    "    for clave, patron in patrones.items():\n",
    "        coincidencias = re.search(patron, cadena)\n",
    "        if coincidencias:\n",
    "            valores[clave] = coincidencias.group(1)\n",
    "    \n",
    "    return valores\n",
    "\n",
    "\n",
    "def procesar_diccionarios(items_parte):\n",
    "    # Utiliza una expresión regular para encontrar todas las coincidencias de los diccionarios dentro de corchetes.\n",
    "    diccionarios = re.findall(r'\\{[^}]+\\}', items_parte)\n",
    "\n",
    "    # Inicializa una lista para almacenar los diccionarios procesados.\n",
    "    resultado = []\n",
    "\n",
    "    # Define una función para procesar cada diccionario.\n",
    "    def procesar_diccionario(diccionario_str):\n",
    "        # Utiliza una expresión regular para extraer los valores de las claves deseadas.\n",
    "        item_id_match = re.search(r'\"item_id\": \"([^\"]+)\"', diccionario_str)\n",
    "        item_name_match = re.search(r'\"item_name\": \"([^\"]+)\"', diccionario_str)\n",
    "        playtime_forever_match = re.search(r'\"playtime_forever\": (\\d+)', diccionario_str)\n",
    "        playtime_2weeks_match = re.search(r'\"playtime_2weeks\": (\\d+)', diccionario_str)\n",
    "\n",
    "        # Verifica si se encontró una coincidencia para cada clave antes de extraer el valor.\n",
    "        item_id = item_id_match.group(1) if item_id_match else None\n",
    "        item_name = item_name_match.group(1) if item_name_match else None\n",
    "        playtime_forever = int(playtime_forever_match.group(1)) if playtime_forever_match else None\n",
    "        playtime_2weeks = int(playtime_2weeks_match.group(1)) if playtime_2weeks_match else None\n",
    "\n",
    "        # Crea un diccionario con los valores extraídos.\n",
    "        diccionario_resultado = {\n",
    "            \"item_id\": item_id,\n",
    "            \"item_name\": item_name,\n",
    "            \"playtime_forever\": playtime_forever,\n",
    "            \"playtime_2weeks\": playtime_2weeks\n",
    "        }\n",
    "\n",
    "        return diccionario_resultado\n",
    "\n",
    "    # Procesa cada diccionario encontrado y agrégalo a la lista de resultados.\n",
    "    for diccionario_str in diccionarios:\n",
    "        resultado.append(procesar_diccionario(diccionario_str))\n",
    "\n",
    "    # El resultado es una lista de diccionarios.\n",
    "    return resultado\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "items = pd.DataFrame(columns=['user_id', 'items_count','steam_id','user_url','items'])\n",
    "with open('../dataset/australian_users_items.json', 'r', encoding='utf-8') as file:\n",
    "    data_list = []\n",
    "    for linea in file:\n",
    "        linea = reemplazar_nan_con_none(linea)\n",
    "        linea = linea.replace('\\'', '\\\"')\n",
    "        resultado = re.search(r'\"items\":\\s*(.+)', linea)\n",
    "        linea = extraer_valores(linea)\n",
    "        items_parte = resultado.group(1)\n",
    "        items_parte = procesar_diccionarios(items_parte)\n",
    "        linea['items'] = items_parte \n",
    "        data_list.append(linea)\n",
    "\n",
    "items = pd.DataFrame(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id\n",
       "X03-Suits            3\n",
       "76561198027488037    3\n",
       "76561198100326818    3\n",
       "76561198309337430    3\n",
       "76561198051777058    3\n",
       "                    ..\n",
       "8392158              1\n",
       "76561198056804863    1\n",
       "SparklezTheTurtle    1\n",
       "76561198019707497    1\n",
       "edward_tremethick    1\n",
       "Name: count, Length: 87626, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Revisando registros duplicados\n",
    "items['user_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar registros duplicados\n",
    "items = items.drop_duplicates(subset=['user_id']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función cuyo proposito es desanidar el contenido de la columna `items`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desanidar_items(df):\n",
    "    i = 0\n",
    "    data_list = []\n",
    "    while i <= len(df['user_id']) - 1:\n",
    "        user_id = df['user_id'].iloc[i]\n",
    "        steam_id = df['steam_id'].iloc[i]\n",
    "        user_url = df['user_url'].iloc[i]\n",
    "        lista = df['items'].iloc[i]\n",
    "        for j in lista:\n",
    "            j['user_id'] = user_id\n",
    "            j['steam_id'] = steam_id\n",
    "            j['user_url'] = user_url\n",
    "            data_list.append(j)\n",
    "        i = i + 1\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creación de un nuevo Dataframe `n_items`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_items = pd.DataFrame(desanidar_items(items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_items[['item_id']] = n_items[['item_id']].astype(int)\n",
    "n_items = n_items.drop(['user_url'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_items.to_parquet('../dataset/items.parquet', engine='pyarrow', compression='snappy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creacion de DataFrame para alimentar la función `PlayTimeGenre` la cual se almacenara en el archivo `ranking_genre.parquet` con el objeto de economizar en espacio de memoria RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No ejecutar esta celda en render ya que se consume alrededor de 2 Gb de memoria y la versión gratis tiene solo 512 Mb\n",
    "# Separar lista de generos\n",
    "games_exploded = games.explode('genres')\n",
    "# Reemplaza las cadenas que no sean fechas válidas por NaN\n",
    "games_exploded['release_date'] = pd.to_datetime(games_exploded['release_date'], errors='coerce')\n",
    "#Crear columna year\n",
    "games_exploded['year'] = games_exploded['release_date'].dt.year\n",
    "# Combinar DF's games_explode con items coincidiendo por id    \n",
    "merged_data = games_exploded.merge(n_items, left_on='id', right_on='item_id', how='inner')\n",
    "# Sumar playtime_forever por género y año\n",
    "genre_playtime = merged_data.groupby(['genres','year'])['playtime_forever'].sum().reset_index()\n",
    "# Ordenar de mayor a menor sobre 'playtime_forever'\n",
    "ranking_genre = genre_playtime.sort_values(by='playtime_forever', ascending=False)\n",
    "ranking_genre = ranking_genre.reset_index(drop=True)\n",
    "# Guardar df en formato parquet\n",
    "ranking_genre.to_parquet('../dataset/ranking_genre.parquet', engine='pyarrow', compression='snappy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creación de DataFrame para alimentar la funcion `UserForGenre` la cual se almacena en el archivo `user_genre.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear df user_genre.parquet\n",
    "\n",
    "# Creando df con las columnas 'genres', 'playtime_forever' y 'user_id_y'\n",
    "usersXgenre = merged_data[['genres','playtime_forever','user_id','year']]\n",
    "# Agrupando por usuario y genero\n",
    "usersXgenre = usersXgenre.groupby(['genres','user_id','year'])['playtime_forever'].sum().reset_index()\n",
    "# Guardar df en formato parquet\n",
    "usersXgenre.to_parquet('../dataset/user_genre.parquet', engine='pyarrow', compression='snappy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
