{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import ast\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reemplazar_nan_con_none(cadena):\n",
    "    resultado = \"\"\n",
    "    indice = 0\n",
    "    longitud = len(cadena)\n",
    "\n",
    "    while indice < longitud:\n",
    "        if cadena[indice:indice+3] == \"NaN\":\n",
    "            resultado += \"\\\"NaN\\\"\"\n",
    "            indice += 3\n",
    "        else:\n",
    "            resultado += cadena[indice]\n",
    "            indice += 1\n",
    "\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corregir_comillas(cadena):\n",
    "    # Buscar comillas simples incorrectamente formateadas dentro de la cadena\n",
    "    partes = cadena.split('\"')\n",
    "    for i in range(1, len(partes), 2):\n",
    "        partes[i] = partes[i].replace(\"'\", '\"')\n",
    "    # Volver a unir las partes corregidas\n",
    "    cadena_corregida = '\"'.join(partes)\n",
    "    return cadena_corregida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_valores(cadena):\n",
    "    valores = {}\n",
    "    \n",
    "    # Expresiones regulares para cada clave\n",
    "    patrones = {\n",
    "        'user_id': r'\"user_id\"\\s*:\\s*\"([^\"]+)\"',\n",
    "        'items_count': r'\"items_count\"\\s*:\\s*([^,]+)',\n",
    "        'steam_id': r'\"steam_id\"\\s*:\\s*\"([^\"]+)\"',\n",
    "        'user_url': r'\"user_url\"\\s*:\\s*\"([^\"]+)\"',\n",
    "        'items': r'\"items\"\\s*:\\s*\\{([^}]+)\\}'\n",
    "    }\n",
    "    \n",
    "    for clave, patron in patrones.items():\n",
    "        coincidencias = re.search(patron, cadena)\n",
    "        if coincidencias:\n",
    "            valores[clave] = coincidencias.group(1)\n",
    "    \n",
    "    return valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesar_diccionarios(items_parte):\n",
    "    # Utiliza una expresión regular para encontrar todas las coincidencias de los diccionarios dentro de corchetes.\n",
    "    diccionarios = re.findall(r'\\{[^}]+\\}', items_parte)\n",
    "\n",
    "    # Inicializa una lista para almacenar los diccionarios procesados.\n",
    "    resultado = []\n",
    "\n",
    "    # Define una función para procesar cada diccionario.\n",
    "    def procesar_diccionario(diccionario_str):\n",
    "        # Utiliza una expresión regular para extraer los valores de las claves deseadas.\n",
    "        item_id_match = re.search(r'\"item_id\": \"([^\"]+)\"', diccionario_str)\n",
    "        item_name_match = re.search(r'\"item_name\": \"([^\"]+)\"', diccionario_str)\n",
    "        playtime_forever_match = re.search(r'\"playtime_forever\": (\\d+)', diccionario_str)\n",
    "        playtime_2weeks_match = re.search(r'\"playtime_2weeks\": (\\d+)', diccionario_str)\n",
    "\n",
    "        # Verifica si se encontró una coincidencia para cada clave antes de extraer el valor.\n",
    "        item_id = item_id_match.group(1) if item_id_match else None\n",
    "        item_name = item_name_match.group(1) if item_name_match else None\n",
    "        playtime_forever = int(playtime_forever_match.group(1)) if playtime_forever_match else None\n",
    "        playtime_2weeks = int(playtime_2weeks_match.group(1)) if playtime_2weeks_match else None\n",
    "\n",
    "        # Crea un diccionario con los valores extraídos.\n",
    "        diccionario_resultado = {\n",
    "            \"item_id\": item_id,\n",
    "            \"item_name\": item_name,\n",
    "            \"playtime_forever\": playtime_forever,\n",
    "            \"playtime_2weeks\": playtime_2weeks\n",
    "        }\n",
    "\n",
    "        return diccionario_resultado\n",
    "\n",
    "    # Procesa cada diccionario encontrado y agrégalo a la lista de resultados.\n",
    "    for diccionario_str in diccionarios:\n",
    "        resultado.append(procesar_diccionario(diccionario_str))\n",
    "\n",
    "    # El resultado es una lista de diccionarios.\n",
    "    return resultado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para convertir fechas al formato \"YYYY-MM-DD\"\n",
    "def convertir_fecha(fecha):\n",
    "    # Utilizar expresión regular para extraer componentes de la fecha\n",
    "    match = re.match(r\"Posted (\\w+) (\\d+), (\\d+)\", fecha)\n",
    "    if match:\n",
    "        mes_str, dia_str, anio_str = match.groups()\n",
    "        # Mapear nombres de meses a números\n",
    "        meses = {\n",
    "            'January': '01', 'February': '02', 'March': '03', 'April': '04',\n",
    "            'May': '05', 'June': '06', 'July': '07', 'August': '08',\n",
    "            'September': '09', 'October': '10', 'November': '11', 'December': '12'\n",
    "        }\n",
    "        # Crear la fecha en el nuevo formato\n",
    "        nueva_fecha = f\"{anio_str}-{meses[mes_str]}-{dia_str.zfill(2)}\"\n",
    "        return nueva_fecha\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargar australian_user_reviews.json y convertirlo a parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../dataset/australian_user_reviews.json', 'r', encoding='utf-8') as file:\n",
    "    data_list = []\n",
    "    for linea in file:\n",
    "        linea = reemplazar_nan_con_none(linea)\n",
    "        data = ast.literal_eval(linea.strip())\n",
    "        if isinstance(data, dict):\n",
    "            data_list.append(data)\n",
    "    reviews = pd.DataFrame(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['user_id'].value_counts() #Validar duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.drop_duplicates(subset=['user_id']) #Eliminar duplicados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función analisis de sentimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "def analyze_sentiment(lista):\n",
    "    nlist =[]\n",
    "    for i in lista:\n",
    "        sentiment = sia.polarity_scores(i['review'])    \n",
    "        try:               \n",
    "            if sentiment['compound'] >= 0.05:\n",
    "                i['sentiment_analysis'] = 2  # Positivo\n",
    "                del i['review']\n",
    "            elif sentiment['compound'] <= -0.05:\n",
    "                i['sentiment_analysis'] = 0  # Malo\n",
    "                del i['review']\n",
    "            else:\n",
    "                i['sentiment_analysis'] = 1  # Neutral\n",
    "                del i['review']\n",
    "        except:\n",
    "            i['sentiment_analysis'] = 1\n",
    "            del i['review']\n",
    "        nlist.append(i)\n",
    "    return nlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['reviews'] = reviews['reviews'].apply(lambda x: analyze_sentiment(x))   # Aplicando NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desanidar_reviews(df):\n",
    "    i = 0\n",
    "    data_list = []\n",
    "    while i <= len(df['user_id']) -1:\n",
    "        user_id = df['user_id'].iloc[i]\n",
    "        user_url = df['user_url'].iloc[i]\n",
    "        lista = df['reviews'].iloc[i]\n",
    "        for j in lista:\n",
    "            j['user_id'] = user_id\n",
    "            j['user_url'] = user_url\n",
    "            data_list.append(j)\n",
    "        i = i + 1\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_reviews = pd.DataFrame(desanidar_reviews(reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar la función de conversión a la columna y crear una nueva columna con las fechas reformateadas\n",
    "n_reviews['posted_date'] = n_reviews['posted'].apply(convertir_fecha)\n",
    "n_reviews['posted_date'] = pd.to_datetime(n_reviews['posted_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_reviews['posted_date'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guarda DF en archivo parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_reviews.to_parquet('../dataset/australian_user_reviews.parquet', engine='pyarrow', compression='snappy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargar output_steam_games.json y convertirlo a parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../dataset/output_steam_games.json', 'r', encoding='utf-8') as file:\n",
    "    data_list = []\n",
    "    for linea in file:\n",
    "        linea = linea.replace('\"NaN\"', '')\n",
    "        data = json.loads(linea.strip())\n",
    "        if isinstance(data, dict):\n",
    "            data_list.append(data)\n",
    "    games = pd.DataFrame(data_list)\n",
    "    games['price'] = games['price'].replace('Free To Play', 0)\n",
    "    games['price'] = pd.to_numeric(games['price'], errors='coerce')\n",
    "    games['metascore'] = pd.to_numeric(games['metascore'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games['id'].value_counts()         # Validando registros duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = games.drop_duplicates(subset=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputar cero a valores nulos de las columnas 'price' y 'discount_price'\n",
    "games[['price','discount_price']] = games[['price','discount_price']].fillna(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar nulos para aquellos registros que tengan regitros nulo en las columnas 'title', 'app_name' e 'id\n",
    "games = games.dropna(subset=['title', 'app_name','id'], how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardar output_steam_games.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games.to_parquet('../dataset/output_steam_games.parquet', engine='pyarrow', compression='snappy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar australian_users_items.json y convertirlo a parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.DataFrame(columns=['user_id', 'items_count','steam_id','user_url','items'])\n",
    "with open('../dataset/australian_users_items.json', 'r', encoding='utf-8') as file:\n",
    "    data_list = []\n",
    "    for linea in file:\n",
    "        linea = linea.replace('\"NaN\"', '')\n",
    "        linea = linea.replace('\\'', '\\\"')\n",
    "        resultado = re.search(r'\"items\":\\s*(.+)', linea)\n",
    "        linea = extraer_valores(linea)\n",
    "        items_parte = resultado.group(1)\n",
    "        items_parte = procesar_diccionarios(items_parte)\n",
    "        linea['items'] = items_parte \n",
    "        data_list.append(linea)\n",
    "\n",
    "items = pd.DataFrame(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items['user_id'].value_counts()         #Validando registros duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = items.drop_duplicates(subset=['user_id'])       # Eliminar duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desanidar_items(df):\n",
    "    i = 0\n",
    "    data_list = []\n",
    "    while i <= len(df['user_id']) - 1:\n",
    "        user_id = df['user_id'].iloc[i]\n",
    "        steam_id = df['steam_id'].iloc[i]\n",
    "        user_url = df['user_url'].iloc[i]\n",
    "        lista = df['items'].iloc[i]\n",
    "        for j in lista:\n",
    "            j['user_id'] = user_id\n",
    "            j['steam_id'] = steam_id\n",
    "            j['user_url'] = user_url\n",
    "            data_list.append(j)\n",
    "        i = i + 1\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_items = pd.DataFrame(desanidar_items(items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_items.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardar australian_users_items.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_items.to_parquet('../dataset/australian_users_items.parquet', engine='pyarrow', compression='snappy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generar ranking_genre.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No ejecutar esta celda en render ya que se consume alrededor de 2 Gb de memoria y la versión gratis tiene solo 512 Mb\n",
    "# Separar lista de generos\n",
    "games_exploded = games.explode('genres')\n",
    "# Combinar DF's games_explode con items coincidiendo por id    \n",
    "merged_data = games_exploded.merge(items, left_on='id', right_on='item_id', how='inner')\n",
    "# Sumar playtime_forever por género, este dataframe supera el gigabyte de memoria, por ello es necesario almacenar el df ranking_genre\n",
    "genre_playtime = merged_data.groupby('genres')['playtime_forever'].sum().reset_index()\n",
    "# Ordenar de mayor a menor sobre 'playtime_forever'\n",
    "ranking_genre = genre_playtime.sort_values(by='playtime_forever', ascending=False)\n",
    "ranking_genre = ranking_genre.reset_index(drop=True)\n",
    "# Guardar df en formato parquet\n",
    "ranking_genre.to_parquet('../dataset/ranking_genre.parquet', engine='pyarrow', compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear df user_genre.parquet\n",
    "\n",
    "# Creando df con las columnas 'genres', 'playtime_forever' y 'user_id_y'\n",
    "usersXgenre = merged_data[['genres','playtime_forever','user_id_y','user_url']]\n",
    "# Agrupando por usuario y genero\n",
    "usersXgenre = usersXgenre.groupby(['genres','user_id_y','user_url'])['playtime_forever'].sum().reset_index()\n",
    "# Guardar df en formato parquet\n",
    "usersXgenre.to_parquet('../dataset/user_genre.parquet', engine='pyarrow', compression='snappy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
